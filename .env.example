# Vega2.0 example environment configuration
# Copy this file to .env and fill in values. Never commit secrets.

# API key required for both API and CLI (sent via X-API-Key header)
API_KEY=CHANGE_ME_VEGA_API_KEY

# Bind host/port for FastAPI (keep on localhost for security)
HOST=127.0.0.1
PORT=8000

# Default LLM model name for Ollama or HF
# For Ollama: e.g., mistral, llama3, phi3
# For HF: use fully qualified model id when backend is hf
MODEL_NAME=mistral

# Backend toggle: "ollama" or "hf"
# Currently only ollama implemented; hf stubbed for later
LLM_BACKEND=ollama

# Optional: Slack webhook URL for integrations testing
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX
