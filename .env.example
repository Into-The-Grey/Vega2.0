# .env.example - Vega2.0 configuration template
# Copy to .env and fill in your values

API_KEY=changeme
HOST=127.0.0.1
PORT=8000
MODEL_NAME=llama3
LLM_BACKEND=ollama

# Optional: Slack integration
SLACK_WEBHOOK_URL=

# Optional: Advanced caching (Redis cluster/distributed cache)
REDIS_MODE=standalone   # standalone, cluster, sentinel
REDIS_CLUSTER_NODES=    # host1:port1,host2:port2,... (for cluster mode)
REDIS_USERNAME=
REDIS_PASSWORD=
REDIS_DB=0
REDIS_SSL=false

# Optional: Resilience and tuning
LLM_TIMEOUT_SEC=60
LLM_MAX_RETRIES=2
LLM_RETRY_BACKOFF=0.5
BREAKER_FAIL_THRESHOLD=5
BREAKER_RESET_SECONDS=30
CACHE_TTL_SECONDS=60
MAX_RESPONSE_CHARS=4000
MAX_PROMPT_CHARS=4000
RETENTION_DAYS=0
PII_MASKING=false
API_KEYS_EXTRA=
LOG_LEVEL=INFO

# Generation defaults
GEN_TEMPERATURE=0.7
GEN_TOP_P=0.9
GEN_TOP_K=40
GEN_REPEAT_PENALTY=1.1
GEN_PRESENCE_PENALTY=0.0
GEN_FREQUENCY_PENALTY=0.0
GEN_DYNAMIC=false
