# Vega LLM Configuration
# Language model settings and behavior controls

# Model Configuration
model_name: "llama3"
base_url: "http://localhost:11434"
api_key: null  # Set if using remote API
timeout: 60
max_retries: 3
retry_backoff: 0.5

# Context & Generation
context_window: 4096
max_tokens: 2048
streaming: true

# Content Moderation & Ethics
behavior:
  content_moderation:
    censorship_level: "moderate"      # none, light, moderate, strict
    ethical_guidelines: "balanced"    # permissive, balanced, strict  
    vulgarity_filter: "moderate"      # none, light, moderate, strict
    profanity_filter: "moderate"      # none, light, moderate, strict
  
  response_style:
    personality: "helpful"            # professional, helpful, casual, creative
    verbosity: "balanced"             # concise, balanced, detailed, verbose
    formality: "moderate"             # informal, moderate, formal
    humor_level: "light"              # none, light, moderate, high
  
  safety_ethics:
    refuse_harmful: true
    refuse_illegal: true
    refuse_violence: true
    refuse_adult_content: true
    refuse_personal_info: true
  
  creativity_flexibility:
    creativity_level: "moderate"      # conservative, moderate, creative, experimental
    follow_instructions: "strict"     # flexible, moderate, strict
    challenge_assumptions: false
    ask_clarification: true
  
  model_parameters:
    temperature: 0.7                  # 0.0-2.0, higher = more creative/random
    top_p: 0.9                       # 0.0-1.0, nucleus sampling
    frequency_penalty: 0.0            # -2.0-2.0, reduce repetition
    presence_penalty: 0.0             # -2.0-2.0, encourage new topics
  
  advanced:
    system_prompt_prefix: ""
    custom_instructions: ""
    context_awareness: "high"         # low, moderate, high
    memory_retention: "session"       # none, session, persistent

# Circuit Breaker & Resilience
circuit_breaker:
  failure_threshold: 5
  reset_timeout: 30
  half_open_max_calls: 3

# Caching
cache:
  enabled: true
  ttl: 60
  max_size: 1000

# Safety Limits
safety:
  max_prompt_length: 4000
  max_response_length: 4000
  rate_limit_per_user: 10
  content_filtering: true
