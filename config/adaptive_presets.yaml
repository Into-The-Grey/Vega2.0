# Adaptive Federated Learning Preset Configurations
# =================================================

# Research Configuration (Experimental and Comprehensive)
# -------------------------------------------------------
# Use when: Experimenting with new techniques or research scenarios
# Trade-offs: May be unstable, includes all experimental features
research:
  initial_algorithm: "scaffold"
  adaptation_enabled: true
  min_participants: 2
  max_participants: 50
  
  monitoring:
    history_size: 200
    performance_window: 15
    thresholds:
      performance_degradation: 0.05  # More sensitive
      convergence_stagnation_variance: 0.0005
      high_latency_ms: 300
      
  hyperparameter_optimization:
    enabled: true
    exploration_factor: 0.4  # Higher exploration
    learning_rate_bounds: [0.0001, 0.2]
    
  experimental:
    enable_predictive_adaptation: true
    enable_multi_objective_optimization: true
    enable_federated_hyperopt: true
    enable_participant_clustering: true

# Production Configuration (Stable and Reliable)
# ----------------------------------------------
# Use when: Deploying in production environments
# Trade-offs: Conservative settings, proven techniques only
production:
  initial_algorithm: "fedavg"
  adaptation_enabled: true
  min_participants: 5
  max_participants: 15
  
  monitoring:
    history_size: 50
    performance_window: 8
    thresholds:
      performance_degradation: 0.15  # Less sensitive
      convergence_stagnation_variance: 0.002
      high_latency_ms: 800
      
  hyperparameter_optimization:
    enabled: true
    exploration_factor: 0.1  # Conservative exploration
    learning_rate_bounds: [0.005, 0.05]
    
  participant_selection:
    min_reliability_score: 0.8  # Higher reliability requirement
    min_accuracy_threshold: 0.75
    max_training_time: 300
    
  experimental:
    enable_predictive_adaptation: false
    enable_multi_objective_optimization: false
    enable_federated_hyperopt: false
    enable_participant_clustering: false

# Fast Training Configuration (Speed Optimized)
# ---------------------------------------------
# Use when: Quick training cycles are needed
# Trade-offs: May sacrifice some accuracy for speed
fast:
  initial_algorithm: "fedavg"
  adaptation_enabled: true
  min_participants: 3
  max_participants: 8
  
  monitoring:
    history_size: 30
    performance_window: 5
    
  hyperparameter_optimization:
    enabled: true
    exploration_factor: 0.2
    local_epochs_options: [1, 3, 5]  # Fewer epochs
    
  communication:
    compression_enabled: true
    quantization_levels: [8, 16]  # Aggressive compression
    sparsification_ratios: [0.5, 0.9]
    
  participant_selection:
    selection_strategy: "performance_based"
    max_training_time: 120  # Strict time limit
    
  resource_limits:
    max_training_time_per_round: 180
    max_communication_time: 30

# High Quality Configuration (Accuracy Optimized) 
# -----------------------------------------------
# Use when: Maximum model quality is required
# Trade-offs: Slower training, more resource intensive
high_quality:
  initial_algorithm: "scaffold" 
  adaptation_enabled: true
  min_participants: 8
  max_participants: 25
  
  monitoring:
    history_size: 150
    performance_window: 12
    thresholds:
      performance_degradation: 0.08
      convergence_stagnation_variance: 0.0008
      
  hyperparameter_optimization:
    enabled: true
    exploration_factor: 0.25
    learning_rate_bounds: [0.0005, 0.08]
    local_epochs_options: [3, 5, 10, 15]  # More epochs
    
  communication:
    adaptive_protocols: true
    compression_enabled: false  # No compression for quality
    quantization_levels: [32]   # High precision
    
  participant_selection:
    scoring_weights:
      accuracy: 0.4      # Higher accuracy weight
      reliability: 0.3
      contribution: 0.2
      efficiency: 0.1
    min_reliability_score: 0.9
    min_accuracy_threshold: 0.8

# Robust Configuration (Fault Tolerant)
# -------------------------------------
# Use when: Operating in unreliable environments
# Trade-offs: More overhead for fault tolerance
robust:
  initial_algorithm: "fedprox"  # More robust to heterogeneity
  adaptation_enabled: true
  min_participants: 5
  max_participants: 12
  
  monitoring:
    history_size: 80
    performance_window: 10
    anomaly_detection_enabled: true
    thresholds:
      performance_degradation: 0.12
      high_latency_ms: 1000  # More tolerant
      high_packet_loss: 0.08
      
  hyperparameter_optimization:
    enabled: true
    exploration_factor: 0.15  # Conservative
    mu_bounds: [0.01, 0.5]    # FedProx regularization
    
  communication:
    adaptive_protocols: true
    compression_enabled: true
    quantization_levels: [8, 16, 32]
    
    poor_network_thresholds:
      latency_ms: 1500     # Very tolerant
      bandwidth_mbps: 3
      packet_loss_rate: 0.15
      
  participant_selection:
    scoring_weights:
      accuracy: 0.25
      reliability: 0.35  # Higher reliability weight
      contribution: 0.25
      efficiency: 0.15
    min_reliability_score: 0.7
    
  adaptation_rules:
    fedavg_to_fedprox:
      revert_after_rounds: 15  # More patient
    fedprox_to_scaffold:
      revert_after_rounds: 20
      
  resource_limits:
    max_training_time_per_round: 600  # More generous timeouts
    max_communication_time: 120

# Minimal Configuration (Resource Constrained)
# --------------------------------------------
# Use when: Operating with limited computational resources
# Trade-offs: Reduced functionality for lower resource usage
minimal:
  initial_algorithm: "fedavg"
  adaptation_enabled: false  # Disabled to save resources
  min_participants: 2
  max_participants: 5
  
  monitoring:
    history_size: 20
    performance_window: 5
    anomaly_detection_enabled: false
    
  hyperparameter_optimization:
    enabled: false  # Disabled to save computation
    
  communication:
    adaptive_protocols: false
    compression_enabled: true
    quantization_levels: [8]  # Aggressive compression
    sparsification_ratios: [0.9]
    
  participant_selection:
    selection_strategy: "random"  # Simpler selection
    
  logging:
    performance_metrics: false  # Reduced logging
    network_conditions: false
    
  resource_limits:
    max_memory_usage_gb: 2
    max_gpu_memory_gb: 1
    max_training_time_per_round: 120
    max_communication_time: 30
    
  experimental:
    enable_predictive_adaptation: false
    enable_multi_objective_optimization: false
    enable_federated_hyperopt: false
    enable_participant_clustering: false

# Edge Computing Configuration (IoT/Mobile Optimized)
# ---------------------------------------------------
# Use when: Deploying on edge devices or mobile networks
# Trade-offs: Optimized for bandwidth and battery constraints
edge:
  initial_algorithm: "fedavg"
  adaptation_enabled: true
  min_participants: 3
  max_participants: 10
  
  monitoring:
    history_size: 40
    performance_window: 6
    thresholds:
      high_latency_ms: 2000   # Very tolerant for mobile
      low_bandwidth_mbps: 1   # Very low bandwidth
      
  communication:
    adaptive_protocols: true
    compression_enabled: true
    quantization_levels: [4, 8]      # Very aggressive compression
    sparsification_ratios: [0.95, 0.98]  # Extreme sparsification
    
    poor_network_thresholds:
      latency_ms: 3000
      bandwidth_mbps: 0.5     # Sub-megabit tolerance
      packet_loss_rate: 0.2
      
  hyperparameter_optimization:
    enabled: true
    local_epochs_options: [1, 2, 3]  # Minimal local computation
    
  participant_selection:
    scoring_weights:
      accuracy: 0.2
      reliability: 0.2
      contribution: 0.2
      efficiency: 0.4  # Prioritize efficiency on edge
      
  resource_limits:
    max_memory_usage_gb: 1
    max_gpu_memory_gb: 0.5
    max_training_time_per_round: 60   # Battery conservation
    max_communication_time: 20