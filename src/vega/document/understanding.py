"""
Vega 2.0 Document Understanding AI Module

This module provides advanced document understanding capabilities including:
- Layout analysis and document structure recognition
- Table extraction and formatting preservation
- Form recognition and field extraction
- Multi-modal document analysis (text, images, tables)
- LayoutLM and DocFormer integration for deep document understanding
- OCR integration with layout-aware text extraction

Dependencies:
- transformers: For LayoutLM and DocFormer models
- torch: For neural network processing
- PIL/Pillow: Image processing for document layouts
- opencv-cv2: Computer vision for layout analysis
- pytesseract: OCR capabilities
- pandas: For table data manipulation
- numpy: Numerical processing
"""

import asyncio
import logging
import io
import json
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union
from pathlib import Path

import numpy as np

try:
    from PIL import Image, ImageDraw, ImageFont
    import cv2

    HAS_VISION_LIBS = True
except ImportError:
    Image = ImageDraw = ImageFont = cv2 = None
    HAS_VISION_LIBS = False

try:
    import pytesseract

    HAS_OCR = True
except ImportError:
    pytesseract = None
    HAS_OCR = False

try:
    from transformers import (
        LayoutLMv3Processor,
        LayoutLMv3ForTokenClassification,
        AutoTokenizer,
        AutoModel,
    )
    import torch

    HAS_TRANSFORMERS = True
except ImportError:
    LayoutLMv3Processor = LayoutLMv3ForTokenClassification = None
    AutoTokenizer = AutoModel = torch = None
    HAS_TRANSFORMERS = False

try:
    import pandas as pd

    HAS_PANDAS = True
except ImportError:
    pd = None
    HAS_PANDAS = False

logger = logging.getLogger(__name__)


class DocumentError(Exception):
    """Custom exception for document processing errors"""

    pass


class DocumentType(Enum):
    """Document type classifications"""

    INVOICE = "invoice"
    RECEIPT = "receipt"
    FORM = "form"
    CONTRACT = "contract"
    REPORT = "report"
    ACADEMIC_PAPER = "academic_paper"
    TECHNICAL_DOC = "technical_doc"
    RESUME = "resume"
    LETTER = "letter"
    TABLE = "table"
    PRESENTATION = "presentation"
    UNKNOWN = "unknown"


class LayoutElement(Enum):
    """Layout element types"""

    TITLE = "title"
    HEADING = "heading"
    PARAGRAPH = "paragraph"
    TABLE = "table"
    FIGURE = "figure"
    CAPTION = "caption"
    HEADER = "header"
    FOOTER = "footer"
    SIDEBAR = "sidebar"
    LIST = "list"
    FORM_FIELD = "form_field"
    SIGNATURE = "signature"
    CHECKBOX = "checkbox"
    BARCODE = "barcode"


class ExtractionMethod(Enum):
    """Text extraction methods"""

    OCR = "ocr"
    LAYOUTLM = "layoutlm"
    HYBRID = "hybrid"
    RULE_BASED = "rule_based"


@dataclass
class BoundingBox:
    """Bounding box coordinates"""

    x1: int
    y1: int
    x2: int
    y2: int

    @property
    def width(self) -> int:
        return self.x2 - self.x1

    @property
    def height(self) -> int:
        return self.y2 - self.y1

    @property
    def center(self) -> Tuple[int, int]:
        return ((self.x1 + self.x2) // 2, (self.y1 + self.y2) // 2)

    @property
    def area(self) -> int:
        return self.width * self.height


@dataclass
class ExtractedElement:
    """Extracted document element"""

    element_type: LayoutElement
    text: str
    bbox: BoundingBox
    confidence: float = 0.0
    properties: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ExtractedTable:
    """Extracted table structure"""

    bbox: BoundingBox
    rows: int
    cols: int
    cells: List[List[str]]
    headers: Optional[List[str]] = None
    confidence: float = 0.0


@dataclass
class FormField:
    """Extracted form field"""

    field_name: str
    field_value: str
    field_type: str
    bbox: BoundingBox
    confidence: float = 0.0


@dataclass
class DocumentConfig:
    """Configuration for document understanding"""

    use_layoutlm: bool = True
    use_ocr: bool = True
    ocr_lang: str = "eng"
    dpi: int = 300
    max_image_size: Tuple[int, int] = (2048, 2048)
    confidence_threshold: float = 0.5
    enable_table_extraction: bool = True
    enable_form_recognition: bool = True


@dataclass
class LayoutConfig:
    """Configuration for layout analysis"""

    min_text_height: int = 10
    max_text_height: int = 200
    line_spacing_threshold: float = 1.5
    paragraph_spacing_threshold: float = 2.0
    column_detection: bool = True
    reading_order_analysis: bool = True


class LayoutAnalyzer:
    """
    Document layout analysis and structure recognition
    """

    def __init__(self, config: LayoutConfig):
        self.config = config

    async def analyze_layout(self, image: np.ndarray) -> List[ExtractedElement]:
        """
        Analyze document layout and identify structural elements

        Args:
            image: Document image as numpy array

        Returns:
            List of extracted layout elements
        """
        try:
            if not HAS_VISION_LIBS:
                logger.warning("Computer vision libraries not available")
                return []

            # Convert to grayscale for analysis
            gray = (
                cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                if len(image.shape) == 3
                else image
            )

            # Detect text regions
            text_regions = await self._detect_text_regions(gray)

            # Classify layout elements
            elements = await self._classify_elements(text_regions, gray)

            # Determine reading order
            if self.config.reading_order_analysis:
                elements = self._sort_reading_order(elements)

            return elements

        except Exception as e:
            logger.error(f"Layout analysis error: {e}")
            return []

    async def _detect_text_regions(self, gray_image: np.ndarray) -> List[BoundingBox]:
        """Detect text regions using computer vision"""
        try:
            # Apply MSER (Maximally Stable Extremal Regions) for text detection
            mser = cv2.MSER_create()
            regions, _ = mser.detectRegions(gray_image)

            # Convert regions to bounding boxes
            bboxes = []
            for region in regions:
                x, y, w, h = cv2.boundingRect(region)

                # Filter by size constraints
                if (
                    self.config.min_text_height <= h <= self.config.max_text_height
                    and w > h * 0.5
                ):  # Minimum aspect ratio
                    bbox = BoundingBox(x, y, x + w, y + h)
                    bboxes.append(bbox)

            # Merge overlapping bounding boxes
            merged_bboxes = self._merge_overlapping_boxes(bboxes)

            return merged_bboxes

        except Exception as e:
            logger.error(f"Text region detection error: {e}")
            return []

    def _merge_overlapping_boxes(
        self, bboxes: List[BoundingBox], overlap_threshold: float = 0.5
    ) -> List[BoundingBox]:
        """Merge overlapping bounding boxes"""
        try:
            if not bboxes:
                return []

            # Sort by y-coordinate then x-coordinate
            sorted_boxes = sorted(bboxes, key=lambda b: (b.y1, b.x1))

            merged = []
            current_box = sorted_boxes[0]

            for next_box in sorted_boxes[1:]:
                # Calculate overlap
                overlap = self._calculate_overlap(current_box, next_box)

                if overlap > overlap_threshold:
                    # Merge boxes
                    current_box = BoundingBox(
                        min(current_box.x1, next_box.x1),
                        min(current_box.y1, next_box.y1),
                        max(current_box.x2, next_box.x2),
                        max(current_box.y2, next_box.y2),
                    )
                else:
                    merged.append(current_box)
                    current_box = next_box

            merged.append(current_box)
            return merged

        except Exception as e:
            logger.debug(f"Box merging error: {e}")
            return bboxes

    def _calculate_overlap(self, box1: BoundingBox, box2: BoundingBox) -> float:
        """Calculate overlap ratio between two bounding boxes"""
        try:
            # Calculate intersection
            x1 = max(box1.x1, box2.x1)
            y1 = max(box1.y1, box2.y1)
            x2 = min(box1.x2, box2.x2)
            y2 = min(box1.y2, box2.y2)

            if x1 >= x2 or y1 >= y2:
                return 0.0

            intersection_area = (x2 - x1) * (y2 - y1)
            union_area = box1.area + box2.area - intersection_area

            return intersection_area / union_area if union_area > 0 else 0.0

        except Exception:
            return 0.0

    async def _classify_elements(
        self, bboxes: List[BoundingBox], image: np.ndarray
    ) -> List[ExtractedElement]:
        """Classify layout elements based on position and characteristics"""
        try:
            elements = []

            for bbox in bboxes:
                # Extract region from image
                region = image[bbox.y1 : bbox.y2, bbox.x1 : bbox.x2]

                # Analyze characteristics
                element_type = self._classify_element_type(bbox, region, image.shape)

                # Extract text using OCR if available
                text = ""
                if HAS_OCR and pytesseract:
                    try:
                        text = pytesseract.image_to_string(region).strip()
                    except Exception as e:
                        logger.debug(f"OCR error: {e}")

                element = ExtractedElement(
                    element_type=element_type,
                    text=text,
                    bbox=bbox,
                    confidence=0.8,  # Default confidence
                )

                elements.append(element)

            return elements

        except Exception as e:
            logger.error(f"Element classification error: {e}")
            return []

    def _classify_element_type(
        self, bbox: BoundingBox, region: np.ndarray, image_shape: Tuple[int, int]
    ) -> LayoutElement:
        """Classify element type based on position and characteristics"""
        try:
            image_height, image_width = image_shape[:2]

            # Position-based classification
            y_ratio = bbox.y1 / image_height
            x_ratio = bbox.x1 / image_width
            width_ratio = bbox.width / image_width
            height_ratio = bbox.height / image_height

            # Header (top 20% of document)
            if y_ratio < 0.2:
                if width_ratio > 0.6:  # Wide element
                    return LayoutElement.HEADER
                elif bbox.height > image_height * 0.05:  # Large text
                    return LayoutElement.TITLE
                else:
                    return LayoutElement.HEADING

            # Footer (bottom 15% of document)
            elif y_ratio > 0.85:
                return LayoutElement.FOOTER

            # Sidebar (narrow columns on edges)
            elif (x_ratio < 0.1 or x_ratio > 0.8) and width_ratio < 0.3:
                return LayoutElement.SIDEBAR

            # Table detection (rectangular structure)
            elif self._is_table_like(region):
                return LayoutElement.TABLE

            # List detection (multiple short lines)
            elif self._is_list_like(region):
                return LayoutElement.LIST

            # Default to paragraph
            else:
                return LayoutElement.PARAGRAPH

        except Exception:
            return LayoutElement.PARAGRAPH

    def _is_table_like(self, region: np.ndarray) -> bool:
        """Detect if region contains table-like structure"""
        try:
            if region.size == 0:
                return False

            # Look for horizontal and vertical lines
            horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 1))
            vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 25))

            horizontal_lines = cv2.morphologyEx(
                region, cv2.MORPH_OPEN, horizontal_kernel
            )
            vertical_lines = cv2.morphologyEx(region, cv2.MORPH_OPEN, vertical_kernel)

            # Count line pixels
            h_line_pixels = np.sum(horizontal_lines > 0)
            v_line_pixels = np.sum(vertical_lines > 0)

            # Threshold for table detection
            total_pixels = region.shape[0] * region.shape[1]
            line_ratio = (h_line_pixels + v_line_pixels) / total_pixels

            return line_ratio > 0.02  # 2% of pixels are lines

        except Exception:
            return False

    def _is_list_like(self, region: np.ndarray) -> bool:
        """Detect if region contains list-like structure"""
        try:
            if region.size == 0:
                return False

            # Analyze horizontal projection (sum of pixels in each row)
            h_projection = np.sum(region, axis=1)

            # Find peaks (text lines) and valleys (spaces)
            peaks = []
            valleys = []

            for i in range(1, len(h_projection) - 1):
                if (
                    h_projection[i] > h_projection[i - 1]
                    and h_projection[i] > h_projection[i + 1]
                ):
                    peaks.append(i)
                elif (
                    h_projection[i] < h_projection[i - 1]
                    and h_projection[i] < h_projection[i + 1]
                ):
                    valleys.append(i)

            # Check for regular spacing (list characteristic)
            if len(peaks) >= 3:
                spacings = [peaks[i + 1] - peaks[i] for i in range(len(peaks) - 1)]
                avg_spacing = np.mean(spacings)
                spacing_variance = np.var(spacings)

                # Regular spacing indicates list
                return spacing_variance < (avg_spacing * 0.5) ** 2

            return False

        except Exception:
            return False

    def _sort_reading_order(
        self, elements: List[ExtractedElement]
    ) -> List[ExtractedElement]:
        """Sort elements in natural reading order"""
        try:
            # Group elements by approximate y-coordinate (lines)
            lines = {}
            line_threshold = 20  # pixels

            for element in elements:
                y_center = element.bbox.center[1]

                # Find existing line or create new one
                matched_line = None
                for line_y in lines.keys():
                    if abs(y_center - line_y) <= line_threshold:
                        matched_line = line_y
                        break

                if matched_line:
                    lines[matched_line].append(element)
                else:
                    lines[y_center] = [element]

            # Sort lines by y-coordinate and elements within lines by x-coordinate
            sorted_elements = []
            for line_y in sorted(lines.keys()):
                line_elements = sorted(lines[line_y], key=lambda e: e.bbox.x1)
                sorted_elements.extend(line_elements)

            return sorted_elements

        except Exception as e:
            logger.debug(f"Reading order sorting error: {e}")
            return elements


class TableExtractor:
    """
    Table detection and extraction from documents
    """

    def __init__(self, config: DocumentConfig):
        self.config = config

    async def extract_tables(
        self, image: np.ndarray, elements: List[ExtractedElement]
    ) -> List[ExtractedTable]:
        """
        Extract table structures from document

        Args:
            image: Document image
            elements: Previously identified layout elements

        Returns:
            List of extracted tables
        """
        try:
            if not HAS_VISION_LIBS:
                logger.warning(
                    "Computer vision libraries not available for table extraction"
                )
                return []

            # Find table elements
            table_elements = [
                e for e in elements if e.element_type == LayoutElement.TABLE
            ]

            tables = []
            for table_element in table_elements:
                table = await self._extract_single_table(image, table_element.bbox)
                if table:
                    tables.append(table)

            # Also try to detect tables not found by layout analysis
            additional_tables = await self._detect_additional_tables(image, elements)
            tables.extend(additional_tables)

            return tables

        except Exception as e:
            logger.error(f"Table extraction error: {e}")
            return []

    async def _extract_single_table(
        self, image: np.ndarray, bbox: BoundingBox
    ) -> Optional[ExtractedTable]:
        """Extract single table from specified region"""
        try:
            # Extract table region
            table_region = image[bbox.y1 : bbox.y2, bbox.x1 : bbox.x2]

            # Convert to grayscale
            if len(table_region.shape) == 3:
                gray_table = cv2.cvtColor(table_region, cv2.COLOR_RGB2GRAY)
            else:
                gray_table = table_region

            # Detect table structure
            rows, cols = self._detect_table_structure(gray_table)

            if rows > 0 and cols > 0:
                # Extract cell contents
                cells = await self._extract_table_cells(gray_table, rows, cols)

                # Determine headers (usually first row)
                headers = cells[0] if cells else None

                return ExtractedTable(
                    bbox=bbox,
                    rows=rows,
                    cols=cols,
                    cells=cells,
                    headers=headers,
                    confidence=0.8,
                )

            return None

        except Exception as e:
            logger.debug(f"Single table extraction error: {e}")
            return None

    def _detect_table_structure(self, gray_table: np.ndarray) -> Tuple[int, int]:
        """Detect table structure (rows and columns)"""
        try:
            if gray_table.size == 0:
                return 0, 0

            # Detect horizontal lines
            horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))
            horizontal_lines = cv2.morphologyEx(
                gray_table, cv2.MORPH_OPEN, horizontal_kernel
            )

            # Detect vertical lines
            vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))
            vertical_lines = cv2.morphologyEx(
                gray_table, cv2.MORPH_OPEN, vertical_kernel
            )

            # Count rows and columns
            h_projection = np.sum(horizontal_lines, axis=1)
            v_projection = np.sum(vertical_lines, axis=0)

            # Find peaks in projections
            h_peaks = self._find_projection_peaks(h_projection)
            v_peaks = self._find_projection_peaks(v_projection)

            rows = len(h_peaks) + 1 if h_peaks else 1
            cols = len(v_peaks) + 1 if v_peaks else 1

            return rows, cols

        except Exception as e:
            logger.debug(f"Table structure detection error: {e}")
            return 0, 0

    def _find_projection_peaks(
        self, projection: np.ndarray, min_prominence: int = 100
    ) -> List[int]:
        """Find peaks in projection array"""
        try:
            peaks = []
            for i in range(1, len(projection) - 1):
                if (
                    projection[i] > projection[i - 1]
                    and projection[i] > projection[i + 1]
                    and projection[i] > min_prominence
                ):
                    peaks.append(i)
            return peaks

        except Exception:
            return []

    async def _extract_table_cells(
        self, gray_table: np.ndarray, rows: int, cols: int
    ) -> List[List[str]]:
        """Extract text content from table cells"""
        try:
            if not HAS_OCR or not pytesseract:
                logger.warning("OCR not available for table cell extraction")
                return []

            # Divide table into cells
            cell_height = gray_table.shape[0] // rows
            cell_width = gray_table.shape[1] // cols

            cells = []

            for row in range(rows):
                cell_row = []
                for col in range(cols):
                    # Calculate cell boundaries
                    y1 = row * cell_height
                    y2 = min((row + 1) * cell_height, gray_table.shape[0])
                    x1 = col * cell_width
                    x2 = min((col + 1) * cell_width, gray_table.shape[1])

                    # Extract cell region
                    cell_region = gray_table[y1:y2, x1:x2]

                    # Extract text using OCR
                    try:
                        cell_text = pytesseract.image_to_string(cell_region).strip()
                        cell_row.append(cell_text)
                    except Exception as e:
                        logger.debug(f"Cell OCR error: {e}")
                        cell_row.append("")

                cells.append(cell_row)

            return cells

        except Exception as e:
            logger.error(f"Table cell extraction error: {e}")
            return []

    async def _detect_additional_tables(
        self, image: np.ndarray, elements: List[ExtractedElement]
    ) -> List[ExtractedTable]:
        """Detect additional tables not found by layout analysis"""
        try:
            # This is a simplified implementation
            # In production, would use more sophisticated table detection
            additional_tables = []

            # Look for regular grid patterns in the image
            gray = (
                cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                if len(image.shape) == 3
                else image
            )

            # Apply Hough line transform to detect grid structures
            edges = cv2.Canny(gray, 50, 150, apertureSize=3)
            lines = cv2.HoughLinesP(
                edges, 1, np.pi / 180, threshold=100, minLineLength=50, maxLineGap=10
            )

            if lines is not None and len(lines) > 10:  # Minimum lines for table
                # Analyze line patterns to identify potential table regions
                # This is a simplified approach
                pass

            return additional_tables

        except Exception as e:
            logger.debug(f"Additional table detection error: {e}")
            return []


class FormRecognizer:
    """
    Form recognition and field extraction
    """

    def __init__(self, config: DocumentConfig):
        self.config = config

    async def recognize_forms(
        self, image: np.ndarray, elements: List[ExtractedElement]
    ) -> List[FormField]:
        """
        Recognize form fields and extract their values

        Args:
            image: Document image
            elements: Previously identified layout elements

        Returns:
            List of extracted form fields
        """
        try:
            fields = []

            # Detect form-like structures
            form_regions = await self._detect_form_regions(image, elements)

            for region in form_regions:
                region_fields = await self._extract_fields_from_region(image, region)
                fields.extend(region_fields)

            return fields

        except Exception as e:
            logger.error(f"Form recognition error: {e}")
            return []

    async def _detect_form_regions(
        self, image: np.ndarray, elements: List[ExtractedElement]
    ) -> List[BoundingBox]:
        """Detect regions that likely contain form fields"""
        try:
            form_regions = []

            # Look for patterns indicating forms:
            # 1. Multiple short text elements aligned vertically
            # 2. Presence of colons, underscores, or boxes
            # 3. Regular spacing patterns

            # Group elements by vertical alignment
            aligned_groups = self._group_aligned_elements(elements)

            for group in aligned_groups:
                if len(group) >= 3:  # Minimum for form section
                    # Check if elements contain form-like text
                    form_like_count = 0
                    for element in group:
                        if self._is_form_like_text(element.text):
                            form_like_count += 1

                    if form_like_count >= len(group) * 0.5:  # 50% form-like
                        # Create bounding box for the group
                        min_x = min(e.bbox.x1 for e in group)
                        min_y = min(e.bbox.y1 for e in group)
                        max_x = max(e.bbox.x2 for e in group)
                        max_y = max(e.bbox.y2 for e in group)

                        form_regions.append(BoundingBox(min_x, min_y, max_x, max_y))

            return form_regions

        except Exception as e:
            logger.debug(f"Form region detection error: {e}")
            return []

    def _group_aligned_elements(
        self, elements: List[ExtractedElement]
    ) -> List[List[ExtractedElement]]:
        """Group elements that are vertically aligned"""
        try:
            groups = []
            alignment_threshold = 50  # pixels

            for element in elements:
                # Find existing group or create new one
                matched_group = None
                for group in groups:
                    # Check alignment with group
                    avg_x = sum(e.bbox.x1 for e in group) / len(group)
                    if abs(element.bbox.x1 - avg_x) <= alignment_threshold:
                        matched_group = group
                        break

                if matched_group:
                    matched_group.append(element)
                else:
                    groups.append([element])

            return groups

        except Exception as e:
            logger.debug(f"Element grouping error: {e}")
            return []

    def _is_form_like_text(self, text: str) -> bool:
        """Check if text appears to be form-related"""
        try:
            if not text:
                return False

            form_indicators = [
                ":",
                "_",
                "___",
                "[]",
                "()",
                "name",
                "date",
                "address",
                "phone",
                "email",
                "signature",
                "amount",
                "total",
                "company",
                "department",
                "title",
                "position",
            ]

            text_lower = text.lower()

            for indicator in form_indicators:
                if indicator in text_lower:
                    return True

            # Check for patterns like "Field: ____"
            if ":" in text and ("_" in text or len(text.strip()) < 50):
                return True

            return False

        except Exception:
            return False

    async def _extract_fields_from_region(
        self, image: np.ndarray, region: BoundingBox
    ) -> List[FormField]:
        """Extract form fields from a specific region"""
        try:
            fields = []

            # Extract region from image
            region_img = image[region.y1 : region.y2, region.x1 : region.x2]

            if not HAS_OCR or not pytesseract:
                return fields

            # Use OCR to extract text with bounding boxes
            ocr_data = pytesseract.image_to_data(
                region_img, output_type=pytesseract.Output.DICT
            )

            # Process OCR results to identify field patterns
            for i in range(len(ocr_data["text"])):
                text = ocr_data["text"][i].strip()
                if text and len(text) > 2:
                    # Calculate absolute coordinates
                    x = region.x1 + ocr_data["left"][i]
                    y = region.y1 + ocr_data["top"][i]
                    w = ocr_data["width"][i]
                    h = ocr_data["height"][i]

                    bbox = BoundingBox(x, y, x + w, y + h)
                    conf = ocr_data["conf"][i] / 100.0

                    # Try to identify field type and extract value
                    field_info = self._analyze_field_text(text)

                    if field_info:
                        field = FormField(
                            field_name=field_info["name"],
                            field_value=field_info["value"],
                            field_type=field_info["type"],
                            bbox=bbox,
                            confidence=conf,
                        )
                        fields.append(field)

            return fields

        except Exception as e:
            logger.debug(f"Field extraction error: {e}")
            return []

    def _analyze_field_text(self, text: str) -> Optional[Dict[str, str]]:
        """Analyze text to determine field name, value, and type"""
        try:
            # Look for field patterns like "Name: John Doe" or "Amount: $100"
            if ":" in text:
                parts = text.split(":", 1)
                if len(parts) == 2:
                    field_name = parts[0].strip()
                    field_value = parts[1].strip()
                    field_type = self._determine_field_type(field_name, field_value)

                    return {
                        "name": field_name,
                        "value": field_value,
                        "type": field_type,
                    }

            # Look for patterns like "Name ________" or "Amount: _____"
            if "_" in text:
                # This indicates an empty field
                field_name = text.replace("_", "").strip()
                return {
                    "name": field_name,
                    "value": "",
                    "type": self._determine_field_type(field_name, ""),
                }

            return None

        except Exception:
            return None

    def _determine_field_type(self, field_name: str, field_value: str) -> str:
        """Determine field type based on name and value"""
        try:
            name_lower = field_name.lower()
            value_lower = field_value.lower()

            # Email detection
            if "email" in name_lower or "e-mail" in name_lower or "@" in field_value:
                return "email"

            # Phone detection
            if (
                "phone" in name_lower
                or "tel" in name_lower
                or any(char.isdigit() for char in field_value)
            ):
                return "phone"

            # Date detection
            if "date" in name_lower or "/" in field_value or "-" in field_value:
                return "date"

            # Amount/Currency detection
            if (
                "amount" in name_lower
                or "price" in name_lower
                or "total" in name_lower
                or "$" in field_value
                or "€" in field_value
                or "£" in field_value
            ):
                return "currency"

            # Address detection
            if (
                "address" in name_lower
                or "street" in name_lower
                or "city" in name_lower
            ):
                return "address"

            # Name detection
            if "name" in name_lower or "title" in name_lower:
                return "name"

            return "text"

        except Exception:
            return "text"


class DocumentStructureAnalyzer:
    """
    High-level document structure analysis
    """

    def __init__(self, config: DocumentConfig):
        self.config = config

    async def analyze_structure(self, image: np.ndarray) -> Dict[str, Any]:
        """
        Analyze overall document structure

        Args:
            image: Document image

        Returns:
            Dictionary containing structure analysis results
        """
        try:
            # Determine document type
            doc_type = await self._classify_document_type(image)

            # Analyze reading flow
            reading_flow = await self._analyze_reading_flow(image)

            # Detect document regions
            regions = await self._detect_document_regions(image)

            # Calculate layout metrics
            metrics = self._calculate_layout_metrics(image, regions)

            return {
                "document_type": doc_type,
                "reading_flow": reading_flow,
                "regions": regions,
                "metrics": metrics,
                "confidence": 0.8,
            }

        except Exception as e:
            logger.error(f"Structure analysis error: {e}")
            return {
                "document_type": DocumentType.UNKNOWN,
                "reading_flow": "unknown",
                "regions": [],
                "metrics": {},
                "confidence": 0.0,
            }

    async def _classify_document_type(self, image: np.ndarray) -> DocumentType:
        """Classify document type based on layout patterns"""
        try:
            # This is a simplified classification
            # In production, would use ML models trained on document types

            height, width = image.shape[:2]
            aspect_ratio = width / height

            # Analyze layout characteristics
            if not HAS_VISION_LIBS:
                return DocumentType.UNKNOWN

            gray = (
                cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                if len(image.shape) == 3
                else image
            )

            # Look for table structures
            horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))
            vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))

            horizontal_lines = cv2.morphologyEx(gray, cv2.MORPH_OPEN, horizontal_kernel)
            vertical_lines = cv2.morphologyEx(gray, cv2.MORPH_OPEN, vertical_kernel)

            table_score = (
                np.sum(horizontal_lines > 0) + np.sum(vertical_lines > 0)
            ) / (height * width)

            if table_score > 0.05:
                return DocumentType.TABLE

            # Check aspect ratio for different document types
            if aspect_ratio > 1.4:  # Wide format
                return DocumentType.PRESENTATION
            elif aspect_ratio < 0.8:  # Tall format
                return DocumentType.REPORT
            else:
                return DocumentType.FORM

        except Exception as e:
            logger.debug(f"Document type classification error: {e}")
            return DocumentType.UNKNOWN

    async def _analyze_reading_flow(self, image: np.ndarray) -> str:
        """Analyze document reading flow pattern"""
        try:
            # Simplified reading flow analysis
            # Would use more sophisticated analysis in production

            # Detect columns
            if not HAS_VISION_LIBS:
                return "single_column"

            gray = (
                cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                if len(image.shape) == 3
                else image
            )

            # Analyze vertical white spaces to detect columns
            v_projection = np.sum(
                gray < 200, axis=0
            )  # Sum of dark pixels in each column

            # Find valleys (column separators)
            valleys = []
            threshold = np.max(v_projection) * 0.1

            for i in range(1, len(v_projection) - 1):
                if (
                    v_projection[i] < threshold
                    and v_projection[i] < v_projection[i - 1]
                    and v_projection[i] < v_projection[i + 1]
                ):
                    valleys.append(i)

            if len(valleys) >= 1:
                return "multi_column"
            else:
                return "single_column"

        except Exception as e:
            logger.debug(f"Reading flow analysis error: {e}")
            return "unknown"

    async def _detect_document_regions(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """Detect different regions in document (header, body, footer, etc.)"""
        try:
            regions = []
            height, width = image.shape[:2]

            # Define standard regions based on position
            regions.append(
                {
                    "type": "header",
                    "bbox": BoundingBox(0, 0, width, int(height * 0.2)),
                    "confidence": 0.8,
                }
            )

            regions.append(
                {
                    "type": "body",
                    "bbox": BoundingBox(0, int(height * 0.2), width, int(height * 0.8)),
                    "confidence": 0.9,
                }
            )

            regions.append(
                {
                    "type": "footer",
                    "bbox": BoundingBox(0, int(height * 0.8), width, height),
                    "confidence": 0.8,
                }
            )

            return regions

        except Exception as e:
            logger.debug(f"Region detection error: {e}")
            return []

    def _calculate_layout_metrics(
        self, image: np.ndarray, regions: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """Calculate layout quality metrics"""
        try:
            height, width = image.shape[:2]

            metrics = {
                "aspect_ratio": width / height,
                "text_density": 0.0,
                "whitespace_ratio": 0.0,
                "alignment_score": 0.0,
            }

            if HAS_VISION_LIBS:
                gray = (
                    cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
                    if len(image.shape) == 3
                    else image
                )

                # Calculate text density
                text_pixels = np.sum(gray < 200)  # Dark pixels assumed to be text
                total_pixels = height * width
                metrics["text_density"] = text_pixels / total_pixels

                # Calculate whitespace ratio
                white_pixels = np.sum(gray > 240)  # Very light pixels
                metrics["whitespace_ratio"] = white_pixels / total_pixels

                # Simple alignment score based on edge detection
                edges = cv2.Canny(gray, 50, 150)
                vertical_edges = np.sum(edges, axis=0)
                alignment_peaks = len(
                    [
                        i
                        for i in range(1, len(vertical_edges) - 1)
                        if vertical_edges[i] > np.max(vertical_edges) * 0.1
                    ]
                )
                metrics["alignment_score"] = min(1.0, alignment_peaks / 10.0)

            return metrics

        except Exception as e:
            logger.debug(f"Layout metrics calculation error: {e}")
            return {
                "aspect_ratio": 1.0,
                "text_density": 0.0,
                "whitespace_ratio": 0.0,
                "alignment_score": 0.0,
            }


class DocumentUnderstandingAI:
    """
    Main document understanding system integrating all components
    """

    def __init__(self, config: Optional[DocumentConfig] = None):
        self.config = config or DocumentConfig()

        # Initialize components
        self.layout_config = LayoutConfig()
        self.layout_analyzer = LayoutAnalyzer(self.layout_config)
        self.table_extractor = TableExtractor(self.config)
        self.form_recognizer = FormRecognizer(self.config)
        self.structure_analyzer = DocumentStructureAnalyzer(self.config)

        # Initialize ML models if available
        self.layoutlm_processor = None
        self.layoutlm_model = None

        if HAS_TRANSFORMERS and self.config.use_layoutlm:
            try:
                self.layoutlm_processor = LayoutLMv3Processor.from_pretrained(
                    "microsoft/layoutlmv3-base"
                )
                self.layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(
                    "microsoft/layoutlmv3-base"
                )
                logger.info("LayoutLM models loaded successfully")
            except Exception as e:
                logger.warning(f"Failed to load LayoutLM models: {e}")

    async def understand_document(
        self,
        image_path: Union[str, Path],
        extract_tables: bool = True,
        recognize_forms: bool = True,
    ) -> Dict[str, Any]:
        """
        Comprehensive document understanding

        Args:
            image_path: Path to document image
            extract_tables: Whether to extract tables
            recognize_forms: Whether to recognize forms

        Returns:
            Complete document understanding results
        """
        try:
            # Load image
            if not HAS_VISION_LIBS:
                logger.error("Vision libraries not available")
                return {}

            image = cv2.imread(str(image_path))
            if image is None:
                logger.error(f"Could not load image: {image_path}")
                return {}

            # Convert BGR to RGB
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Resize if too large
            height, width = image.shape[:2]
            max_width, max_height = self.config.max_image_size

            if width > max_width or height > max_height:
                scale = min(max_width / width, max_height / height)
                new_width = int(width * scale)
                new_height = int(height * scale)
                image = cv2.resize(
                    image, (new_width, new_height), interpolation=cv2.INTER_AREA
                )

            # Analyze document structure
            structure = await self.structure_analyzer.analyze_structure(image)

            # Analyze layout
            elements = await self.layout_analyzer.analyze_layout(image)

            # Extract tables if requested
            tables = []
            if extract_tables and self.config.enable_table_extraction:
                tables = await self.table_extractor.extract_tables(image, elements)

            # Recognize forms if requested
            form_fields = []
            if recognize_forms and self.config.enable_form_recognition:
                form_fields = await self.form_recognizer.recognize_forms(
                    image, elements
                )

            # Use LayoutLM if available
            layoutlm_results = {}
            if self.layoutlm_processor and self.layoutlm_model:
                layoutlm_results = await self._apply_layoutlm(image, elements)

            # Compile results
            results = {
                "image_info": {
                    "original_size": (width, height),
                    "processed_size": image.shape[:2],
                    "path": str(image_path),
                },
                "document_structure": structure,
                "layout_elements": [
                    {
                        "type": elem.element_type.value,
                        "text": elem.text,
                        "bbox": {
                            "x1": elem.bbox.x1,
                            "y1": elem.bbox.y1,
                            "x2": elem.bbox.x2,
                            "y2": elem.bbox.y2,
                        },
                        "confidence": elem.confidence,
                    }
                    for elem in elements
                ],
                "tables": [
                    {
                        "bbox": {
                            "x1": table.bbox.x1,
                            "y1": table.bbox.y1,
                            "x2": table.bbox.x2,
                            "y2": table.bbox.y2,
                        },
                        "rows": table.rows,
                        "cols": table.cols,
                        "cells": table.cells,
                        "headers": table.headers,
                        "confidence": table.confidence,
                    }
                    for table in tables
                ],
                "form_fields": [
                    {
                        "name": field.field_name,
                        "value": field.field_value,
                        "type": field.field_type,
                        "bbox": {
                            "x1": field.bbox.x1,
                            "y1": field.bbox.y1,
                            "x2": field.bbox.x2,
                            "y2": field.bbox.y2,
                        },
                        "confidence": field.confidence,
                    }
                    for field in form_fields
                ],
                "layoutlm_results": layoutlm_results,
                "processing_config": {
                    "use_layoutlm": self.config.use_layoutlm,
                    "use_ocr": self.config.use_ocr,
                    "extract_tables": extract_tables,
                    "recognize_forms": recognize_forms,
                },
            }

            return results

        except Exception as e:
            logger.error(f"Document understanding error: {e}")
            return {"error": str(e), "image_path": str(image_path)}

    async def _apply_layoutlm(
        self, image: np.ndarray, elements: List[ExtractedElement]
    ) -> Dict[str, Any]:
        """Apply LayoutLM model for enhanced understanding"""
        try:
            if not self.layoutlm_processor or not self.layoutlm_model:
                return {}

            # Convert image to PIL format
            pil_image = Image.fromarray(image)

            # Prepare text and boxes for LayoutLM
            words = []
            boxes = []

            for element in elements:
                if element.text:
                    # Split text into words and create bounding boxes
                    element_words = element.text.split()
                    word_width = element.bbox.width / max(len(element_words), 1)

                    for i, word in enumerate(element_words):
                        words.append(word)
                        # Approximate word position within element
                        word_x1 = element.bbox.x1 + int(i * word_width)
                        word_x2 = element.bbox.x1 + int((i + 1) * word_width)
                        boxes.append(
                            [word_x1, element.bbox.y1, word_x2, element.bbox.y2]
                        )

            if not words:
                return {}

            # Process with LayoutLM
            encoding = self.layoutlm_processor(
                pil_image,
                words,
                boxes=boxes,
                return_tensors="pt",
                truncation=True,
                padding=True,
            )

            # Run inference
            with torch.no_grad():
                outputs = self.layoutlm_model(**encoding)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_labels = torch.argmax(predictions, dim=-1)

            # Process results
            results = {
                "entities": [],
                "confidence_scores": predictions.max(dim=-1)[0].tolist(),
                "model_info": "LayoutLMv3",
            }

            # Map predictions back to words (simplified)
            for i, (word, label_id) in enumerate(
                zip(words, predicted_labels[0].tolist())
            ):
                if i < len(boxes):
                    results["entities"].append(
                        {
                            "word": word,
                            "label_id": label_id,
                            "bbox": boxes[i],
                            "confidence": float(predictions[0][i].max()),
                        }
                    )

            return results

        except Exception as e:
            logger.warning(f"LayoutLM processing error: {e}")
            return {}

    def create_demo_document(self) -> Dict[str, Any]:
        """Create a demo document understanding result"""
        try:
            return {
                "image_info": {
                    "original_size": (2480, 3508),  # A4 at 300 DPI
                    "processed_size": (1240, 1754),
                    "path": "/demo/sample_document.png",
                },
                "document_structure": {
                    "document_type": DocumentType.FORM,
                    "reading_flow": "single_column",
                    "regions": [
                        {"type": "header", "confidence": 0.9},
                        {"type": "body", "confidence": 0.95},
                        {"type": "footer", "confidence": 0.8},
                    ],
                    "metrics": {
                        "aspect_ratio": 0.71,
                        "text_density": 0.35,
                        "whitespace_ratio": 0.45,
                        "alignment_score": 0.8,
                    },
                },
                "layout_elements": [
                    {
                        "type": "title",
                        "text": "Application Form",
                        "bbox": {"x1": 100, "y1": 50, "x2": 1140, "y2": 100},
                        "confidence": 0.95,
                    },
                    {
                        "type": "form_field",
                        "text": "Full Name: ________________",
                        "bbox": {"x1": 100, "y1": 150, "x2": 600, "y2": 180},
                        "confidence": 0.9,
                    },
                    {
                        "type": "form_field",
                        "text": "Email: ____________________",
                        "bbox": {"x1": 100, "y1": 200, "x2": 600, "y2": 230},
                        "confidence": 0.9,
                    },
                ],
                "tables": [
                    {
                        "bbox": {"x1": 100, "y1": 300, "x2": 1140, "y2": 500},
                        "rows": 3,
                        "cols": 3,
                        "cells": [
                            ["Item", "Quantity", "Price"],
                            ["Product A", "2", "$50.00"],
                            ["Product B", "1", "$25.00"],
                        ],
                        "headers": ["Item", "Quantity", "Price"],
                        "confidence": 0.85,
                    }
                ],
                "form_fields": [
                    {
                        "name": "Full Name",
                        "value": "",
                        "type": "name",
                        "bbox": {"x1": 200, "y1": 150, "x2": 600, "y2": 180},
                        "confidence": 0.9,
                    },
                    {
                        "name": "Email",
                        "value": "",
                        "type": "email",
                        "bbox": {"x1": 150, "y1": 200, "x2": 600, "y2": 230},
                        "confidence": 0.9,
                    },
                ],
                "layoutlm_results": {
                    "entities": [
                        {"word": "Application", "label_id": 0, "confidence": 0.95},
                        {"word": "Form", "label_id": 0, "confidence": 0.95},
                        {"word": "Name", "label_id": 1, "confidence": 0.9},
                        {"word": "Email", "label_id": 1, "confidence": 0.9},
                    ],
                    "model_info": "LayoutLMv3",
                },
                "processing_config": {
                    "use_layoutlm": True,
                    "use_ocr": True,
                    "extract_tables": True,
                    "recognize_forms": True,
                },
            }

        except Exception as e:
            logger.error(f"Demo document creation error: {e}")
            return {}


# Example usage and testing
if __name__ == "__main__":

    async def demo():
        """Demo document understanding AI"""
        try:
            # Create document understanding system
            config = DocumentConfig(
                use_layoutlm=HAS_TRANSFORMERS,
                use_ocr=HAS_OCR,
                enable_table_extraction=True,
                enable_form_recognition=True,
            )

            doc_ai = DocumentUnderstandingAI(config)

            print("Document Understanding AI Demo")
            print("=" * 50)
            print(f"LayoutLM available: {HAS_TRANSFORMERS}")
            print(f"OCR available: {HAS_OCR}")
            print(f"Computer Vision available: {HAS_VISION_LIBS}")
            print()

            # Create demo results
            print("Creating demo document analysis...")
            demo_results = doc_ai.create_demo_document()

            print(
                f"Document Type: {demo_results['document_structure']['document_type']}"
            )
            print(f"Layout Elements: {len(demo_results['layout_elements'])}")
            print(f"Tables Found: {len(demo_results['tables'])}")
            print(f"Form Fields: {len(demo_results['form_fields'])}")

            # Display some results
            print("\nLayout Elements:")
            for elem in demo_results["layout_elements"]:
                print(f"  - {elem['type']}: {elem['text'][:50]}...")

            print("\nForm Fields:")
            for field in demo_results["form_fields"]:
                print(
                    f"  - {field['name']} ({field['type']}): {field['value'] or '[empty]'}"
                )

            if demo_results["tables"]:
                table = demo_results["tables"][0]
                print(f"\nTable ({table['rows']}x{table['cols']}):")
                if table["headers"]:
                    print(f"  Headers: {', '.join(table['headers'])}")
                print(f"  Data rows: {table['rows'] - 1}")

            print("\nDocument Understanding AI demo completed successfully!")

        except Exception as e:
            print(f"Demo error: {e}")

    # Run demo
    try:
        asyncio.run(demo())
    except KeyboardInterrupt:
        print("\nDemo stopped by user")
