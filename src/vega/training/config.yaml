# Advanced Training Configuration for Vega2.0
# Enhanced with A/B testing, performance optimization, and knowledge harvesting

# Model Configuration
model_name: "mistralai/Mistral-7B-v0.1"  # HuggingFace model ID or local path
model_type: "causal_lm"                   # causal_lm, seq2seq, etc.
trust_remote_code: false

# Training Strategy
training_strategy: "lora"  # lora, qlora, full, freeze
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: false

# Output and Data Paths
output_dir: "./training/output"
train_file: "./datasets/output.jsonl"
validation_file: "./datasets/validation.jsonl"  # Optional
test_file: "./datasets/test.jsonl"              # Optional

# Training Hyperparameters
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 2e-4
weight_decay: 0.01
num_train_epochs: 3
max_steps: -1
warmup_steps: 100
max_seq_length: 2048

# LoRA/PEFT Configuration
lora:
  enabled: true
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Evaluation and Monitoring
evaluation_strategy: "steps"
eval_steps: 250
save_strategy: "steps"
save_steps: 250
logging_steps: 10
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
early_stopping_patience: 3

# A/B Testing Configuration
ab_testing:
  enabled: false
  models:
    - "mistralai/Mistral-7B-v0.1"
    - "microsoft/DialoGPT-medium"
  strategies:
    - "lora"
    - "qlora"
    - "full"

# Advanced Training Features
gradient_checkpointing: true
fp16: false
bf16: true
deepspeed_config: null  # Path to DeepSpeed config file if using
dataloader_num_workers: 4
remove_unused_columns: false

# Monitoring and Logging
monitoring:
  wandb_enabled: false
  wandb_project: "vega-training"
  wandb_entity: null
  report_to: []  # ["wandb", "tensorboard"]

# Knowledge Extraction and Data Quality
knowledge_extraction_enabled: true
conversation_quality_threshold: 0.7

# Data Preprocessing and Augmentation
data:
  preprocessing:
    normalize_whitespace: true
    add_special_tokens: true
    min_prompt_length: 10
    min_response_length: 20
    max_prompt_length: 1000
    max_response_length: 1500
  augmentation: false

# Performance Optimization
optimization:
  gradient_clipping: 1.0
  lr_scheduler: "cosine"
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8

# Hardware Configuration
hardware:
  use_cpu: false
  device_map: "auto"
  low_cpu_mem_usage: true
  torch_compile: false

# Experimental Features
experimental:
  flash_attention: false
  cpu_offload: false
  disk_offload: false
  mixed_precision: "bf16"

# Evaluation Metrics
evaluation:
  compute_bleu: true
  compute_rouge: true
  compute_perplexity: true
  custom_metrics: ["response_length", "conversation_quality"]

# Model Serving Configuration (for deployment)
serving:
  quantize_for_inference: true
  optimize_for_inference: true
  export_onnx: false
  export_tensorrt: false
